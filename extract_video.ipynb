{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP3rU9jxbIca"
   },
   "source": [
    "Video Gesture Recognition with MediaPipe\n",
    "---CSCE 5280\n",
    "---Team members:\n",
    "-------Solomon Ubani ( solomonubani@my.unt.edu ) \n",
    "       Sulav Poudyal ( sulav697@gmail.com ) \n",
    "       Yen Pham ( yenpham@my.unt.edu ) \n",
    "       Khoa Ho ( khoaho@my.unt.edu ) \n",
    "       Stephanie Brooks( StephanieBrooks2@my.unt.edu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_bVSztgwSAw",
    "outputId": "8c6d7e9a-2d23-4cd3-9769-ddebe7f6e228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.8.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.4 MB 78 kB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Installing collected packages: mediapipe\n",
      "Successfully installed mediapipe-0.8.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2HrDvPOwUvc"
   },
   "outputs": [],
   "source": [
    "# Computer Vision features\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils\n",
    "import mediapipe as mp\n",
    "\n",
    "# Data processing\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File path & temporal processes\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN_LDHSzwWS6"
   },
   "outputs": [],
   "source": [
    "# Set the holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Set the drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeCvf9yxwYPT"
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \n",
    "    # Converts the image color.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    \n",
    "    # Prevents image write\n",
    "    image.flags.writeable = False                  \n",
    "    \n",
    "    # Makes the prediction.\n",
    "    results = model.process(image)\n",
    "    \n",
    "    # Enables image write.\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Convert back to BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    \n",
    "    # Return the image and prediction results.\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAHEjlMJwZvh"
   },
   "outputs": [],
   "source": [
    "# draw_landmarks: Takes a frame and results then applies the landmark \n",
    "# visualizations to hand and pose.\n",
    "def draw_landmarks(image, results):\n",
    "    \n",
    "    # Draw left hand points.                    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, \n",
    "                              mp_holistic.HAND_CONNECTIONS)\n",
    "    # Draw right hand points.\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, \n",
    "                              mp_holistic.HAND_CONNECTIONS) \n",
    "    \n",
    "    # Draw pose points.\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, \n",
    "                              mp_holistic.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgVxumHbwdnX"
   },
   "outputs": [],
   "source": [
    "# extract_keypoints: Gets the x,y,z coordinates of the keypoints of a frame and returns a concatenated\n",
    "# array of those coordinates for the pose, left, and right hand.\n",
    "def extract_keypoints(results):\n",
    "\n",
    "    # Gets and flattens the coordinates for each of the landmark areas. If there\n",
    "    # are no values for the frame, 0's are returned.\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for \n",
    "                     res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for \n",
    "                   res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for \n",
    "                   res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # Returns the concatenated np array for each of the landmarks.\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF3Sdxx7wfbL"
   },
   "outputs": [],
   "source": [
    "# extractKPFromVid: Performs keypoints extraction on each frame of the input video\n",
    "# and saves the keypoints to a numpy array folder\n",
    "def extractKPFromVid(videoMP4,action):\n",
    "  with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # START\n",
    "    print(\"START KP extraction from vid: \\n\")\n",
    "    # Sets count.\n",
    "    count = 0\n",
    "    \n",
    "    # Set the numpyList\n",
    "    numpyList = []\n",
    "\n",
    "    # Sets the videoFile name. **INCLUDE** the extension\n",
    "    videoFile = videoMP4\n",
    "\n",
    "    # Captures the video\n",
    "    cap = cv2.VideoCapture(videoMP4)\n",
    "\n",
    "    # Set the framerate \n",
    "    frameRate = cap.get(5)\n",
    "\n",
    "    print(\"framerate: \",frameRate,\"\\n\")\n",
    "\n",
    "    # While the video is running, read in the video frames\n",
    "    # and extract the keypoints to a file. \n",
    "    while(cap.isOpened()):\n",
    "      print(\"Start video processing. . .\")\n",
    "      # Sets the frame number\n",
    "      frameId = cap.get(1)\n",
    "\n",
    "      # Reads in the frame.\n",
    "      ret, frame = cap.read()\n",
    "      print(ret)\n",
    "      # Display the video frame with landmarks overlaid.\n",
    "      #cv2_imshow(frame)\n",
    "\n",
    "      # If there are no more frames, the capturing stops.\n",
    "      # Otherwise, the next frame is read in.\n",
    "      if (ret != True):\n",
    "        print(\"End of video: \",videoMP4)\n",
    "        break\n",
    "\n",
    "      # ---- Extract and append the keypoints----.\n",
    "      # Keypoints detections.\n",
    "      image,results = mediapipe_detection(frame,holistic)\n",
    "\n",
    "      # Increment count\n",
    "      count+=1\n",
    "\n",
    "      #---Export keypoints---\n",
    "      # Get the keypoints array\n",
    "      keypoints = extract_keypoints(results)\n",
    "\n",
    "      # Append the keypoints to the numpyList.\n",
    "      if(count<89):\n",
    "        numpyList.append(keypoints)     \n",
    "        break\n",
    "\n",
    "    # Stops the capture.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "\n",
    "    # Output finish message.\n",
    "    print (\"Frame Capture Finished.\")\n",
    "\n",
    "    # Return the list of numpy arrays.\n",
    "    return numpyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ass9p7_hwr3J"
   },
   "outputs": [],
   "source": [
    "# Set the folder path for the numpy arrays.\n",
    "DATA_PATH = os.path.join('/content/Data')\n",
    "\n",
    "# Read in the text file to a dataframe.\n",
    "#df_gestures = pd.read_csv(\"/content/Data/labelmap.txt\",sep=\",\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er8im5BW2Ckx"
   },
   "source": [
    "Video keypoint Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcNChuJOQ8Eg"
   },
   "outputs": [],
   "source": [
    "# Set the gestures, window, and sequences.\n",
    "gestures = ['Down','Left','Right','Up']\n",
    "window,sequences = [],[]\n",
    " \n",
    "# Creates the map for gesture to classification values.\n",
    "gestureLabelMap = {label:num for num,label in enumerate(gestures)}\n",
    "\n",
    "# Loops through each video per gesture, processes the video keypoints,\n",
    "#  and builds a list of concatenated numpy arrays.\n",
    "for gesture in gestures:\n",
    "\n",
    "  for root,dirs,files in os.walk(os.path.join(DATA_PATH,gesture)):\n",
    "    print(os.path.join(DATA_PATH,gesture))\n",
    "    for fil in files:\n",
    "      # Extract frames and set the list of numpy arrays from the video.\n",
    "      window = extractKPFromVid(os.path.join(DATA_PATH,gesture,fil),gesture)\n",
    "\n",
    "  # Append the numpyLists for the gesture.\n",
    "  sequences.append(window)\n",
    "  print(\"Sequence complete for: \",gesture,\"\\nSEQUENCE: \",sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCN4HTPB7NxW",
    "outputId": "cef1b0d4-1eb8-479f-e1db-2c60bac9131f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epCe_EVTPs-Q",
    "outputId": "3ca21e9f-650e-4f2a-f6b6-e804fc99c442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 30, 258)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set X to list of numpy arrays.\n",
    "X = np.array(sequences)\n",
    "\n",
    "# Output X dimensions.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSUalwF7Gg8c"
   },
   "outputs": [],
   "source": [
    "# Set the y value\n",
    "y=to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8RNao6BTsa1"
   },
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAkpnu87Tnxt"
   },
   "outputs": [],
   "source": [
    "# Set the testing/training data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5ZFyhzNURr_"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "154P4WSYf7FZ"
   },
   "outputs": [],
   "source": [
    "# Training tracking\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGfeA3jXUQ_B"
   },
   "outputs": [],
   "source": [
    "# Set the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the LSTM layers. \n",
    "# **Note: input_shape: If X.shape values were [a,b,c] it would just be b,c.\n",
    "# If X.shape outputs (4,30,258), the input_shape is (30,258)**\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,258)))\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "# Add the Dense layers\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(gestures.shape[0],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2zfRzy8b7qn"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# multiclass classification model --> categorical cross entropy used.\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNhrY2R3fjdb"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train,y_train,epochs=1000,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7kTK9Xsga4O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VGR_MP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
